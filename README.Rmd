---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# important

<!-- badges: start -->
[![R-CMD-check](https://github.com/tidymodels/important/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidymodels/important/actions/workflows/R-CMD-check.yaml)
[![Codecov test coverage](https://codecov.io/gh/tidymodels/important/graph/badge.svg)](https://app.codecov.io/gh/tidymodels/important)
<!-- badges: end -->

The important package has a succinct interface for obtaining estimates of predictor importance with tidymodels objects. A few of the main features: 

- Any performance metrics from the yardstick package can be used. 
- Importance can be calculated for either the original columns or at the level of any derived model terms created during feature engineering. 
- The computations that loop across permutation iterations and predictors columns are easily parallelized. 
- The results are returned in a tidy format. 

There are also recipe steps for supervised feature selection: 

- `step_predictors_retain()` can filter the predictors using a single conditional statement (e.g., absolute correlation with the outcome > 0.75, etc). 
- `step_predictors_best()` can retain the most important predictors for the outcome using a single scoring function. 
- `step_predictors_desirability()` retains the most important predictors for the outcome using multiple scoring functions, blended using desirability functions.  

The latter two steps can be tuned over the proportion of predictors to be retained.  

## Installation

You can install the development version of important from [GitHub](https://github.com/) with:

``` r
install.packages("devtools")
# or
pak::pak("tidymodels/important")
```

## Do we really need another package that computes variable importances?

The main reason for making important is censored regression models. tidymodels released tools for fitting and qualifying models that have censored outcomes. This included some dynamic performance metrics that were evaluated at different time points. This was a substantial change for us, and it would have been even more challenging to add to other packages. 

## Variable importance Example

Let's look at an analysis that models [food delivery times](https://aml4td.org/chapters/whole-game.html#sec-delivery-times). The outcome is the time between an order being placed and the delivery (all data are complete - there is no censoring). We model this in terms of the order day/time, the distance to the restaurant, and which items are contained in the order. Exploratory data analysis shows several nonlinear trends in the data and some interactions between these trends.  

We'll load the tidymodels and important packages to get started. 

```{r}
#| label: startup-sshh
#| include: false
library(tidymodels)
library(important)
theme_set(theme_bw())
```
```{r}
#| label: startup
#| include: false
library(tidymodels)
library(important)
```

The data are split into training, validation, and testing sets. 

```{r}
#| label: food-data
data(deliveries, package = "modeldata")

set.seed(991)
delivery_split <- initial_validation_split(deliveries, prop = c(0.6, 0.2), strata = time_to_delivery)
delivery_train <- training(delivery_split)
```

The model uses a recipe with spline terms for the hour and distances. The nonlinear trend over the time of order changes on the day, so we added interactions between these two sets of terms. Finally, a simple linear regression model is used for estimation:  

```{r}
#| label: model
delivery_rec <- 
  recipe(time_to_delivery ~ ., data = delivery_train) |> 
  step_dummy(all_factor_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_spline_natural(hour, distance, deg_free = 10) |> 
  step_interact(~ starts_with("hour_"):starts_with("day_"))

lm_wflow <- workflow(delivery_rec, linear_reg())
lm_fit <- fit(lm_wflow, delivery_train)
```

First, letâ€™s capture the effect of the individual model terms. These terms are from the derived features in the models, such as dummy variables, spline terms, interaction columns, etc. 

```{r}
#| label: derived-importance
set.seed(382)
lm_deriv_imp <- 
  importance_perm(
    lm_fit,
    data = delivery_train,
    metrics = metric_set(mae, rsq),
    times = 50,
    type = "derived"
  )
lm_deriv_imp
```
Using mean absolute error as the metric of interest, the top 5 features are: 

```{r}
lm_deriv_imp |> 
	filter(.metric == "mae") |> 
	slice_max(importance, n = 5)
```

Two notes: 

- The importance scores are the ratio of the mean change in performance and the associated standard error. The mean value is always increasing with importance, no matter which direction is preferred for the specific metric(s). 

- We can run these in parallel by loading the future package and specifying a parallel backend using the `plan()` function.  

There is a plot method that can help visualize the results: 

```{r}
#| label: derived-plot
#| fig.height: 8

autoplot(lm_deriv_imp, top = 50)
```

Since there are spline terms and interactions for the hour column, we might not care about the importance of a term such as `hour_06` (the sixth spline feature). In aggregate, we might want to know the effect of the original predictor columns. The `type` option is used for this purpose:

```{r}
#| label: original-importance
set.seed(382)
lm_orig_imp <- 
	importance_perm(
		lm_fit,
		data = delivery_train,
		metrics = metric_set(mae, rsq),
		times = 50,
		type = "original"
	)

# Top five: 
lm_orig_imp |> 
	filter(.metric == "mae") |> 
	slice_max(importance, n = 5)
```

```{r}
#| label: original-plot

autoplot(lm_orig_imp)
```

## Supervised Feature Selection Example

Using the same dataset, let's illustrate the most common tool for filtering predictors: using random forest importance scores. 

important can use any of the "scoring functions" from the [filtro](https://filtro.tidymodels.org/) package. You can supply one, and the proportion of the predictors  to retain: 

```{r}
#| label: select-top
set.seed(491)
selection_rec <- 
	recipe(time_to_delivery ~ ., data = delivery_train) |> 
	step_predictor_best(all_predictors(), score = "imp_rf", prop_terms = 1/4) |> 
	step_dummy(all_factor_predictors()) |> 
	step_zv(all_predictors()) |> 
	step_spline_natural(any_of(c("hour", "distance")), deg_free = 10) |> 
	step_interact(~ starts_with("hour_"):starts_with("day_")) |> 
	prep()
selection_rec
```

A list of possible scores is contained in the help page for the recipe steps. 

Note that we changed selectors in `step_spline_natural()` to use `any_of()` instead of specific names. Any step downstream of any filtering steps should be generalized so that there is no failure if the columns were removed. Using `any_of()` selects these two columns _if they still remain in the data_.  

Which were removed? 

```{r}
#| label: tidy-filter
selection_res <- 
	tidy(selection_rec, number = 1) |> 
	arrange(desc(score))

selection_res

mean(selection_res$removed)
```

This example shows the basic usage of the recipe. In practice, we would probably do things differently: 

 - This step would be included in a workflow so that it is coupled to a model. 
 - It would be a good idea to optimize how much selection is done by setting `prop_terms = tune()` in the step and using one of the tuning functions to find a good proportion. 
 
*Inappropriate* use of these selection steps occurs when it is used before the data are split or outside of a resampling step.  

## Code of Conduct
  
Please note that the important project is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/1/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.
